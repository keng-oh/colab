{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keng-oh/colab/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cxczINDOFPY",
        "colab_type": "text"
      },
      "source": [
        "# DQN\n",
        "## 用語\n",
        "|用語|意味|\n",
        "|---|---|\n",
        "|環境（environment）|エージェントがおかれた周囲の状況|\n",
        "|エージェント（agent）|\t環境の中で振る舞う主体|\n",
        "|状態  s （situation）|\t絶えず変化する環境の一時点のようす|\n",
        "|行動  a （action）|\tエージェントが環境の中で行う振る舞い|\n",
        "|報酬  R （return）|\tある行動の直後に得られる得点|\n",
        "|収益  G （gain）|\tある行動を選択し､その後のステップすべてで最適な選択をとったとき､将来的に見込まれる報酬の累積値｡累積報酬とも｡|\n",
        "|状態価値  V （value）\t|ある状態から得られる収益の期待値|\n",
        "|行動価値  Q （quality）\t|ある状態である行動をとったときに得られる収益の期待値|\n",
        "|方策  π （policy）|\tエージェントが行動を決定するためのアルゴリズム|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r7VLFFaMvqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ボール\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class Ball:\n",
        "    '''落下してくるボールを定義したクラス'''\n",
        "    \n",
        "    def __init__(self, col):\n",
        "        self.col = col\n",
        "        self.row = 0  # ボールの初期位置は上端で固定\n",
        "\n",
        "    def update(self):\n",
        "        self.row += 1 # 下に1マス移動\n",
        "\n",
        "    def isDroped(self, n_rows): # 画面外に出たかどうか\n",
        "        return True if self.row >= n_rows else False # n_rows-1が下端の位置"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbBvZ0JTZjI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 環境全体\n",
        "\n",
        "class CatchBall:\n",
        "    '''\n",
        "    環境全体を定義したクラス\n",
        "    __init__: インスタンス作成\n",
        "    reset: 環境をリセットする\n",
        "    execute_action: 行動を環境に入力\n",
        "    observe: 状態と報酬、エピソード終了判定を返す\n",
        "    draw: 現在の画像を描画(`observe`で使用)  \n",
        "    '''\n",
        "    \n",
        "    def __init__(self, time_limit=500, size=8, p_len=3,interval=5):\n",
        "        self.screen_n_rows = size  # 画面の横幅\n",
        "        self.screen_n_cols = size  # 画面の縦幅\n",
        "        self.player_length = p_len # 棒の長さ\n",
        "        self.enable_actions = (0, 1, 2)  # 行動のタプル\n",
        "        self.frame_rate = 5  # フレームレート\n",
        "        self.ball_post_interval = interval  # ボールの出現間隔\n",
        "        self.ball_past_time = 0  # 最後にボールが出現してからの経過時間\n",
        "        self.past_time = 0  # ゲームが開始してからの経過時間\n",
        "        self.balls = []  # ボールのリスト\n",
        "        self.time_limit = time_limit  # ゲームを強制的に打ち切るまでの時間\n",
        "\n",
        "        self.reset()  # 環境を初期化\n",
        "    def reset(self):  # 環境の初期化\n",
        "        # 棒の位置を下端のランダムな位置に初期化\n",
        "        self.player_row = self.screen_n_rows - 1\n",
        "        self.player_col = np.random.randint(self.screen_n_cols - self.player_length)\n",
        "\n",
        "        # ボールを1つ生成\n",
        "        self.balls = []\n",
        "        self.balls.append(Ball(np.random.randint(self.screen_n_cols)))\n",
        "\n",
        "        # その他の初期化\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n",
        "        self.past_time = 0\n",
        "        self.ball_past_time = 0\n",
        "\n",
        "    def execute_action(self, action):  # 行動を環境に入力\n",
        "        '''\n",
        "        action:\n",
        "            0: 停止\n",
        "            1: 左へ移動\n",
        "            2: 右へ移動\n",
        "        '''\n",
        "        if action == self.enable_actions[1]:\n",
        "            #  左へ1つ移動\n",
        "            #  左端(0)より小さくはしない\n",
        "            self.player_col = max(0, self.player_col - 1)  \n",
        "        elif action == self.enable_actions[2]:\n",
        "            #  右へ1つ移動\n",
        "            #  右端(self.screen_n_cols - self.player_length)より大きくはしない\n",
        "            self.player_col = min(self.player_col + 1, self.screen_n_cols - self.player_length)\n",
        "        else:\n",
        "            # その場から動かない\n",
        "            pass\n",
        "\n",
        "        # ボールを更新\n",
        "        for b in self.balls:\n",
        "            b.update()\n",
        "            \n",
        "        # ボールが出現してからの経過時間がボールの出現間隔になったら\n",
        "        if self.ball_past_time == self.ball_post_interval:\n",
        "            self.ball_past_time = 0\n",
        "            new_pos = np.random.randint(self.screen_n_cols)  # 新しいボールの位置を乱数で生成\n",
        "            \n",
        "            # 既にボールがある場合は一つ前のボールから最低でも棒の長さ分横軸の離れた位置に生成する\n",
        "            while len(self.balls) > 0 and abs(new_pos - self.balls[-1].col) < self.player_length:\n",
        "                    new_pos = np.random.randint(self.screen_n_cols) # 近い場合は再度乱数を生成\n",
        "            self.balls.append(Ball(new_pos))  # 生成したボールをリストに追加\n",
        "            \n",
        "        self.ball_past_time += 1\n",
        "\n",
        "        self.reward = 0\n",
        "        self.terminal = False # エピソード終了判定\n",
        "\n",
        "        self.past_time += 1\n",
        "        if  self.past_time > self.time_limit: # 強制終了時間を過ぎたら終了\n",
        "            self.terminal = True\n",
        "        # リストは先に生成されたものから順番に入っているので、ボールの接触判定は先頭のもののみで良い\n",
        "        if self.balls[0].row == self.screen_n_rows - 1: \n",
        "            if self.player_col <= self.balls[0].col < self.player_col + self.player_length:\n",
        "                # キャッチ成功\n",
        "                self.reward = 1\n",
        "            else:\n",
        "                # 失敗\n",
        "                self.reward = -1\n",
        "                self.terminal = True  # エピソード終了\n",
        "\n",
        "        new_balls = [] # 次時刻でのボールのリスト\n",
        "        \n",
        "        # 画面中にあるもののみを次時刻に引き継ぐ\n",
        "        for b in self.balls:\n",
        "            if not b.isDroped(self.screen_n_rows):\n",
        "                new_balls.append(b)\n",
        "        self.balls = copy.copy(new_balls)\n",
        "\n",
        "    def observe(self):  # 現在の状態・報酬と、エピソード終了判定を観測\n",
        "        self.draw()  # 環境の画像を描画\n",
        "        return self.screen, self.reward, self.terminal\n",
        "        \n",
        "    def draw(self):  # self.screenに現在の画面を格納する\n",
        "        # 黒(0)で全画面を初期化\n",
        "        self.screen = np.zeros((self.screen_n_rows, self.screen_n_cols))\n",
        "\n",
        "        # 白(1)で棒を描画\n",
        "        self.screen[self.player_row, self.player_col:self.player_col + self.player_length] = 1\n",
        "\n",
        "        # 灰色(0.5)でボールを描画\n",
        "        for b in self.balls:\n",
        "            self.screen[b.row, b.col] = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRtjEFRvZpnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 不要な警告を非表示にする\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ライブラリを読み込み\n",
        "from collections import deque\n",
        "\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras.layers import InputLayer\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import model_from_config\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46o-cjQCZxBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ε-greedyのεを時刻毎に一定値ずつ減衰させる｡\n",
        "# 最低値になったらその値で固定する\n",
        "INITIAL_EXPLORATION = 1.0  # εの初期値\n",
        "FINAL_EXPLORATION = 0.1  # εの最低値\n",
        "EXPLORATION_STEPS = 500  # 最低値になるまでの時間\n",
        "\n",
        "def clone_model(model, custom_objects={}):\n",
        "    '''ターゲットネットワークの更新を行う'''\n",
        "    \n",
        "    config = {\n",
        "        'class_name': model.__class__.__name__,\n",
        "        'config': model.get_config(),\n",
        "    }\n",
        "    clone = model_from_config(config, custom_objects=custom_objects)\n",
        "    clone.set_weights(model.get_weights())\n",
        "    return clone\n",
        "\n",
        "def huber_loss(y_true, y_pred):\n",
        "    '''Huber損失'''\n",
        "    \n",
        "    error = tf.abs(y_pred - y_true)  # TD誤差の絶対値\n",
        "    quadratic_part = tf.clip_by_value(error, 0.0, 1.0)  # クリッピングされた誤差\n",
        "    linear_part = error - quadratic_part  # クリッピングによる減少分 = huber損失の線形部分\n",
        "    loss = tf.reduce_sum(0.5 * tf.square(quadratic_part) + linear_part) # huber損失の計算\n",
        "    tf.summary.scalar('loss', loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    '''\n",
        "    DQNのエージェント｡以下のメソッドを持つ｡\n",
        "    \n",
        "    __init__: インスタンス作成\n",
        "    Q_values: 行動価値関数の推定値を返す\n",
        "    select_action: 行動をε-greedyで決定する\n",
        "    update_exploration: ε-greedyのεを決定\n",
        "    update_target_model: target networkを更新  \n",
        "    store_experience:experience replayのための経験を保存\n",
        "    experience_replay: experience replayを実行  \n",
        "    '''\n",
        "    \n",
        "    def __init__(self, enable_actions, size=8):\n",
        "        self.enable_actions = enable_actions  # とることのできる行動\n",
        "        self.n_actions = len(self.enable_actions)  # 行動の総数 \n",
        "        self.minibatch_size = 32  # バッチサイズ\n",
        "        self.replay_memory_size = 5000  # experience replayの記憶する経験数\n",
        "        self.learning_rate = 0.00025  # 学習率\n",
        "        self.discount_factor = 0.9  # 割引累積報酬の累積率\n",
        "        self.exploration = INITIAL_EXPLORATION  # εの初期値\n",
        "        self.exploration_step = (INITIAL_EXPLORATION - FINAL_EXPLORATION) / EXPLORATION_STEPS  # εの減少量\n",
        "        self.size = size # 画像の大きさ\n",
        "\n",
        "        # experience replayのメモリ\n",
        "        self.D = deque(maxlen=self.replay_memory_size)\n",
        "\n",
        "        # lossを初期化\n",
        "        self.current_loss = 0.0\n",
        "\n",
        "        # DNNの初期化\n",
        "        self.model = Sequential()\n",
        "        self.model.add(InputLayer(input_shape=(self.size,self.size)))  # 画像を入力\n",
        "        self.model.add(Flatten())  # 一次元配列にする\n",
        "        self.model.add(Dense(64, activation='relu'))  # 中間層\n",
        "        self.model.add(Dense(32, activation='relu'))  # 中間層\n",
        "        self.model.add(Dense(self.n_actions, activation='linear', name='main_output'))  # 各行動についてそれぞれ出力\n",
        "\n",
        "        self.model.compile(loss=huber_loss,\n",
        "                           optimizer=RMSprop(lr=self.learning_rate),\n",
        "                           metrics=['accuracy'])\n",
        "        # target networkの初期化\n",
        "        self.target_model = copy.copy(self.model)\n",
        "\n",
        "    def Q_values(self, states, isTarget=False):\n",
        "        '''Q値を返す'''\n",
        "        model = self.target_model if isTarget else self.model  # どちらのネットワークを使うかを選ぶ\n",
        "        res = model.predict(np.array([states]))\n",
        "        return res[0]\n",
        "\n",
        "\n",
        "    def select_action(self, states, epsilon):\n",
        "        '''ε-貪欲法の実装'''\n",
        "        if np.random.rand() <= epsilon:\n",
        "            # ランダムに選択\n",
        "            return np.random.choice(self.enable_actions)\n",
        "        else:\n",
        "            # greedyに選択\n",
        "            return self.enable_actions[np.argmax(self.Q_values(states))]\n",
        "        \n",
        "    def update_exploration(self, num):\n",
        "        '''εを指数関数的に減衰させる'''\n",
        "        if self.exploration > FINAL_EXPLORATION:  # まだεが最低値になっていない\n",
        "            self.exploration -= self.exploration_step  # 減少させる\n",
        "            if self.exploration < FINAL_EXPLORATION:\n",
        "                self.exploration = FINAL_EXPLORATION\n",
        "\n",
        "        \n",
        "    def update_target_model(self):\n",
        "        '''target networkの更新'''\n",
        "        self.target_model = clone_model(self.model)\n",
        "        \n",
        "        \n",
        "    def store_experience(self, states, action, reward, states_1, terminal):\n",
        "        '''メモリに記録'''\n",
        "        self.D.append((states, action, reward, states_1, terminal))# (メモリが満杯のときは古いものから削除)\n",
        "        return (len(self.D) >= self.replay_memory_size) # メモリが満杯かどうかを返す\n",
        "\n",
        "    def experience_replay(self, step):\n",
        "        '''経験再生'''\n",
        "        state_minibatch = []  # 学習に使う状態\n",
        "        y_minibatch = []  # 学習に使う教師データ\n",
        "\n",
        "        # メモリからランダムにサンプリング\n",
        "        minibatch_size = min(len(self.D), self.minibatch_size)\n",
        "        minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)  # 取り出すデータのindex\n",
        "\n",
        "        for j in minibatch_indexes:\n",
        "            state_j, action_j, reward_j, state_j_1, terminal = self.D[j]  # データを取り出す\n",
        "            action_j_index = self.enable_actions.index(action_j)  # indexを取得\n",
        "\n",
        "            \n",
        "            # 教師データの計算\n",
        "            y_j = self.Q_values(state_j)  # 行動価値関数を推定\n",
        "            # 行動価値関数のうち、実際にとった行動の部分を実際の報酬と1ステップ先の推定値を用いた値に更新\n",
        "            if terminal:\n",
        "                y_j[action_j_index] = reward_j  # 終端状態なら収益は即時報酬のみ\n",
        "            else:\n",
        "                v = np.max(self.Q_values(state_j_1, isTarget=True))\n",
        "                y_j[action_j_index] = reward_j + self.discount_factor * v \n",
        "            # 教師データの計算終了\n",
        "\n",
        "            #バッチに追加\n",
        "            state_minibatch.append(state_j)\n",
        "            y_minibatch.append(y_j)\n",
        "\n",
        "\n",
        "        # 学習開始\n",
        "        s = self.model.fit(np.array(state_minibatch),\n",
        "                       np.array(y_minibatch),\n",
        "                       batch_size=minibatch_size,\n",
        "                       epochs=1,\n",
        "                       verbose=0)\n",
        "        # 損失関数を格納\n",
        "        score = self.model.evaluate(np.array(state_minibatch), np.array(y_minibatch), batch_size=minibatch_size,verbose=0)\n",
        "        self.current_loss = score[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tetua8n2Z7sh",
        "colab_type": "code",
        "outputId": "47dfbd67-2aa7-44a6-fe67-f1b86196b512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        }
      },
      "source": [
        "# 経験させるエピソード数\n",
        "n_epochs =300\n",
        "\n",
        "# 画面サイズ\n",
        "size = 6\n",
        "\n",
        "# ボールの落ちる間隔\n",
        "interval = 4\n",
        "\n",
        "# 棒の長さ\n",
        "p_len = 2\n",
        "\n",
        "# 環境、エージェントの初期化\n",
        "env = CatchBall(size=size,interval=interval,p_len=p_len)\n",
        "agent = DQNAgent(env.enable_actions,size=size)\n",
        "\n",
        "win = 0  # キャッチした回数\n",
        "total_frame = 0  # 総フレーム数\n",
        "e = 0  # エピソード数\n",
        "\n",
        "while e < n_epochs:\n",
        "    # 初期化\n",
        "    frame = 0  # このエピソードが始まってからのフレーム数\n",
        "    loss = 0.0  # 損失関数の合計(後で平均をログとして表示する)\n",
        "    Q_max = 0.0  # 各時刻での行動価値関数の最大値の合計(後で平均をログとして表示する)\n",
        "    env.reset()\n",
        "    state_t_1, reward_t, terminal = env.observe()  # 初期状態の観測\n",
        "    win = 0\n",
        "    while not terminal:  # エピソードが終わるまで実行\n",
        "        state_t = state_t_1\n",
        "\n",
        "        # 行動を選択\n",
        "        action_t = agent.select_action(state_t, agent.exploration)\n",
        "        \n",
        "        # 行動を実行\n",
        "        env.execute_action(action_t)\n",
        "\n",
        "        # 環境を観測\n",
        "        state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "        # 経験を保存\n",
        "        start_replay = agent.store_experience(state_t, action_t, reward_t, state_t_1, terminal)\n",
        "\n",
        "        # experience replay\n",
        "        if start_replay:  # メモリが貯まったら\n",
        "            agent.update_exploration(e)  # εを更新\n",
        "            agent.experience_replay(e)\n",
        "\n",
        "        # 200ステップ毎にtarget networkを同期\n",
        "        if total_frame % 200 == 0 and start_replay:\n",
        "            agent.update_target_model()\n",
        "\n",
        "        # ログ用\n",
        "        frame += 1\n",
        "        total_frame += 1\n",
        "        loss += agent.current_loss\n",
        "        Q_max += np.max(agent.Q_values(state_t))\n",
        "        if reward_t == 1:\n",
        "            win += 1\n",
        "\n",
        "    if start_replay:        \n",
        "        # ログを表示\n",
        "        # 実行する時はコメントアウトを外してください。\n",
        "        # print(\"EPOCH: {:03d}/{:03d} | WIN: {:03d} | LOSS: {:.4f} | Q_MAX: {:.4f}\".format(e+1, n_epochs, win, loss / frame, Q_max / frame))\n",
        "        win = 0\n",
        "\n",
        "    # experience replayが始まった時点からエピソードをカウント\n",
        "    if start_replay:\n",
        "        e += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d56f81241e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 環境、エージェントの初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatchBall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mwin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# キャッチした回数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e9e2a14feae4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, enable_actions, size)\u001b[0m\n\u001b[1;32m     68\u001b[0m                            metrics=['accuracy'])\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# target networkの初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mQ_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpickle_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36munpickle_model\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    256\u001b[0m         raise ValueError('You are trying to load a weight file'\n\u001b[1;32m    257\u001b[0m                          \u001b[0;34m' containing {} layers into a model with {} layers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                          \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                          )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 3 layers into a model with 0 layers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GqYzPZMhKmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "env.reset()\n",
        "\n",
        "def init():  # 初期化\n",
        "    state_t_1, reward_t, terminal = env.observe()\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img\n",
        "\n",
        "\n",
        "def animate(step):  # 繰り返し実行する部分\n",
        "    global state_t_1, reward_t, terminal  # 次に実行したときに引き継ぐためにglobal\n",
        "\n",
        "    if terminal:  # エピソード終了ならリセット\n",
        "        env.reset()\n",
        "    else:  # エピソードが続いているなら行動を選択\n",
        "        state_t = state_t_1\n",
        "        action_t = agent.select_action(state_t, 0.0)  # greedy方策(ε=0.0)\n",
        "        env.execute_action(action_t)\n",
        "\n",
        "    # 環境を観測\n",
        "    state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "    # 表示する画像を描画\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR_CCfYJhMPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib nbagg\n",
        "\n",
        "# 動画表示準備\n",
        "fig = plt.figure(figsize=(env.screen_n_rows / 2, env.screen_n_cols / 2),num='CatchBallGame')\n",
        "img = plt.imshow(state_t_1, interpolation=\"none\", cmap=\"gray\")\n",
        "# 動画再生\n",
        "ani = animation.FuncAnimation(fig, animate, init_func=init, interval=(1000 / env.frame_rate), blit=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}